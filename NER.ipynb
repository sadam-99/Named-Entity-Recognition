{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T13:31:39.205667Z",
     "start_time": "2020-05-27T13:31:38.265348Z"
    }
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x9d in position 2588: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-e2b6949ad38e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Data/NER_nlp_corpus.tsv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w+'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriterows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Data/ner_dataset_nlp_corpus.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x9d in position 2588: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "csv.writer(open(\"Data/NER_nlp_corpus.tsv\", 'w+'), delimiter='\\t').writerows(csv.reader(open(\"Data/ner_dataset_nlp_corpus.csv\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T13:38:45.801388Z",
     "start_time": "2020-05-27T13:38:45.745484Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Unable to process file\n",
      "error = too many values to unpack (expected 2)\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-10-e8dd5f68b49d>\", line 17, in tsv_to_json_format\n",
      "    word,entity=line.split('\\t')\n",
      "ValueError: too many values to unpack (expected 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Line====== Word\tTag\n",
      "\n",
      "=====Line====== Thousands\tO\n",
      "\n",
      "=====Line====== of\tO\n",
      "\n",
      "=====Line====== demonstrators\tO\n",
      "\n",
      "=====Line====== have\tO\n",
      "\n",
      "=====Line====== marched\tO\n",
      "\n",
      "=====Line====== through\tO\n",
      "\n",
      "=====Line====== London\tB-geo\n",
      "\n",
      "=====Line====== to\tO\n",
      "\n",
      "=====Line====== protest\tO\n",
      "\n",
      "=====Line====== the\tO\n",
      "\n",
      "=====Line====== war\tO\n",
      "\n",
      "=====Line====== in\tO\n",
      "\n",
      "=====Line====== Iraq\tB-geo\n",
      "\n",
      "=====Line====== and\tO\n",
      "\n",
      "=====Line====== demand\tO\n",
      "\n",
      "=====Line====== the\tO\n",
      "\n",
      "=====Line====== withdrawal\tO\n",
      "\n",
      "=====Line====== of\tO\n",
      "\n",
      "=====Line====== British\tB-gpe\n",
      "\n",
      "=====Line====== troops\tO\n",
      "\n",
      "=====Line====== from\tO\n",
      "\n",
      "=====Line====== that\tO\n",
      "\n",
      "=====Line====== country\tO\n",
      "\n",
      "=====Line====== Families\tO\n",
      "\n",
      "=====Line====== of\tO\n",
      "\n",
      "=====Line====== soldiers\tO\n",
      "\n",
      "=====Line====== killed\tO\n",
      "\n",
      "=====Line====== in\tO\n",
      "\n",
      "=====Line====== the\tO\n",
      "\n",
      "=====Line====== conflict\tO\n",
      "\n",
      "=====Line====== joined\tO\n",
      "\n",
      "=====Line====== the\tO\n",
      "\n",
      "=====Line====== protesters\tO\n",
      "\n",
      "=====Line====== who\tO\n",
      "\n",
      "=====Line====== carried\tO\n",
      "\n",
      "=====Line====== banners\tO\n",
      "\n",
      "=====Line====== with\tO\n",
      "\n",
      "=====Line====== such\tO\n",
      "\n",
      "=====Line====== slogans\tO\n",
      "\n",
      "=====Line====== as\tO\n",
      "\n",
      "=====Line====== \"\"\"\"\tO\n",
      "\n",
      "=====Line====== Bush\tB-per\n",
      "\n",
      "=====Line====== Number\tO\n",
      "\n",
      "=====Line====== One\tO\n",
      "\n",
      "=====Line====== Terrorist\tO\n",
      "\n",
      "=====Line====== \"\"\"\"\tO\n",
      "\n",
      "=====Line====== and\tO\n",
      "\n",
      "=====Line====== \"\"\"\"\tO\n",
      "\n",
      "=====Line====== Stop\tO\n",
      "\n",
      "=====Line====== the\tO\n",
      "\n",
      "=====Line====== Bombings\tO\n",
      "\n",
      "=====Line====== \"\"\"\"\tO\n",
      "\n",
      "=====Line====== They\tO\n",
      "\n",
      "=====Line====== marched\tO\n",
      "\n",
      "=====Line====== from\tO\n",
      "\n",
      "=====Line====== the\tO\n",
      "\n",
      "=====Line====== Houses\tO\n",
      "\n",
      "=====Line====== of\tO\n",
      "\n",
      "=====Line====== Parliament\tO\n",
      "\n",
      "=====Line====== to\tO\n",
      "\n",
      "=====Line====== a\tO\n",
      "\n",
      "=====Line====== rally\tO\n",
      "\n",
      "=====Line====== in\tO\n",
      "\n",
      "=====Line====== Hyde\tB-geo\n",
      "\n",
      "=====Line====== Park\tI-geo\n",
      "\n",
      "=====Line====== Police\tO\n",
      "\n",
      "=====Line====== put\tO\n",
      "\n",
      "=====Line====== the\tO\n",
      "\n",
      "=====Line====== number\tO\n",
      "\n",
      "=====Line====== of\tO\n",
      "\n",
      "=====Line====== marchers\tO\n",
      "\n",
      "=====Line====== at\tO\n",
      "\n",
      "=====Line====== \"10000\"\tO\n",
      "\n",
      "=====Line====== while\tO\n",
      "\n",
      "=====Line====== organizers\tO\n",
      "\n",
      "=====Line====== claimed\tO\n",
      "\n",
      "=====Line====== it\tO\n",
      "\n",
      "=====Line====== was\tO\n",
      "\n",
      "=====Line====== \"100000\"\tO\n",
      "\n",
      "=====Line====== The\tO\n",
      "\n",
      "=====Line====== protest\tO\n",
      "\n",
      "=====Line====== comes\tO\n",
      "\n",
      "=====Line====== on\tO\n",
      "\n",
      "=====Line====== the\tO\n",
      "\n",
      "=====Line====== eve\tO\n",
      "\n",
      "=====Line====== of\tO\n",
      "\n",
      "=====Line====== the\tO\n",
      "\n",
      "=====Line====== annual\tO\n",
      "\n",
      "=====Line====== conference\tO\n",
      "\n",
      "=====Line====== of\tO\n",
      "\n",
      "=====Line====== Britain\tB-geo\n",
      "\n",
      "=====Line====== 's\tO\n",
      "\n",
      "=====Line====== ruling\tO\n",
      "\n",
      "=====Line====== Labor\tB-org\n",
      "\n",
      "=====Line====== Party\tI-org\n",
      "\n",
      "=====Line====== in\tO\n",
      "\n",
      "=====Line====== the\tO\n",
      "\n",
      "=====Line====== southern\tO\n",
      "\n",
      "=====Line====== English\tB-gpe\n",
      "\n",
      "=====Line====== seaside\tO\n",
      "\n",
      "=====Line====== resort\tO\n",
      "\n",
      "=====Line====== of\tO\n",
      "\n",
      "=====Line====== Brighton\tB-geo\n",
      "\n",
      "=====Line====== The\tO\n",
      "\n",
      "=====Line====== party\tO\n",
      "\n",
      "=====Line====== is\tO\n",
      "\n",
      "=====Line====== divided\tO\n",
      "\n",
      "=====Line====== over\tO\n",
      "\n",
      "=====Line====== Britain\tB-gpe\n",
      "\n",
      "=====Line====== 's\tO\n",
      "\n",
      "=====Line====== participation\tO\n",
      "\n",
      "=====Line====== in\tO\n",
      "\n",
      "=====Line====== the\tO\n",
      "\n",
      "=====Line====== Iraq\tB-geo\n",
      "\n",
      "=====Line====== conflict\tO\n",
      "\n",
      "=====Line====== and\tO\n",
      "\n",
      "=====Line====== the\tO\n",
      "\n",
      "=====Line====== continued\tO\n",
      "\n",
      "=====Line====== deployment\tO\n",
      "\n",
      "=====Line====== of\tO\n",
      "\n",
      "=====Line====== \"8\t500\"\tO\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert .tsv file to dataturks json format. \n",
    "import json\n",
    "import logging\n",
    "import sys\n",
    "def tsv_to_json_format(input_path,output_path,unknown_label):\n",
    "    try:\n",
    "        f=open(input_path,'r') # input file\n",
    "        fp=open(output_path, 'w') # output file\n",
    "        data_dict={}\n",
    "        annotations =[]\n",
    "        label_dict={}\n",
    "        s=''\n",
    "        start=0\n",
    "        for line in f:\n",
    "            if line[0:len(line)-1]!='.\\tO':\n",
    "                print(\"=====Line======\", line)\n",
    "                word,entity=line.split('\\t')\n",
    "                s+=word+\" \"\n",
    "                entity=entity[:len(entity)-1]\n",
    "                if entity!=unknown_label:\n",
    "                    if len(entity) != 1:\n",
    "                        d={}\n",
    "                        d['text']=word\n",
    "                        d['start']=start\n",
    "                        d['end']=start+len(word)-1  \n",
    "                        try:\n",
    "                            label_dict[entity].append(d)\n",
    "                        except:\n",
    "                            label_dict[entity]=[]\n",
    "                            label_dict[entity].append(d) \n",
    "                start+=len(word)+1\n",
    "            else:\n",
    "                data_dict['content']=s\n",
    "                s=''\n",
    "                label_list=[]\n",
    "                for ents in list(label_dict.keys()):\n",
    "                    for i in range(len(label_dict[ents])):\n",
    "                        if(label_dict[ents][i]['text']!=''):\n",
    "                            l=[ents,label_dict[ents][i]]\n",
    "                            for j in range(i+1,len(label_dict[ents])): \n",
    "                                if(label_dict[ents][i]['text']==label_dict[ents][j]['text']):  \n",
    "                                    di={}\n",
    "                                    di['start']=label_dict[ents][j]['start']\n",
    "                                    di['end']=label_dict[ents][j]['end']\n",
    "                                    di['text']=label_dict[ents][i]['text']\n",
    "                                    l.append(di)\n",
    "                                    label_dict[ents][j]['text']=''\n",
    "                            label_list.append(l)                          \n",
    "                            \n",
    "                for entities in label_list:\n",
    "                    label={}\n",
    "                    label['label']=[entities[0]]\n",
    "                    label['points']=entities[1:]\n",
    "                    annotations.append(label)\n",
    "                data_dict['annotation']=annotations\n",
    "                annotations=[]\n",
    "                json.dump(data_dict, fp)\n",
    "                fp.write('\\n')\n",
    "                data_dict={}\n",
    "                start=0\n",
    "                label_dict={}\n",
    "    except Exception as e:\n",
    "        logging.exception(\"Unable to process file\" + \"\\n\" + \"error = \" + str(e))\n",
    "        return None\n",
    "tsv_to_json_format(\"Data/NER_DATA_corpus.tsv\",'Data/ner_corpus_260.json','abc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T13:20:23.750691Z",
     "start_time": "2020-05-27T13:20:23.676535Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-i None] [-o None]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\sadam\\AppData\\Roaming\\jupyter\\runtime\\kernel-ef984d21-f439-430a-ab6d-c28964631bdb.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3334: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# Convert json file to spaCy format.\n",
    "import plac\n",
    "import logging\n",
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "@plac.annotations(input_file=(\"Input file\", \"option\", \"i\", str), output_file=(\"Output file\", \"option\", \"o\", str))\n",
    "\n",
    "def main(input_file=None, output_file=None):\n",
    "    try:\n",
    "        training_data = []\n",
    "        lines=[]\n",
    "        with open(input_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        for line in lines:\n",
    "            data = json.loads(line)\n",
    "            text = data['content']\n",
    "            entities = []\n",
    "            for annotation in data['annotation']:\n",
    "                point = annotation['points'][0]\n",
    "                labels = annotation['label']\n",
    "                if not isinstance(labels, list):\n",
    "                    labels = [labels]\n",
    "\n",
    "                for label in labels:\n",
    "                    entities.append((point['start'], point['end'] + 1 ,label))\n",
    "\n",
    "\n",
    "            training_data.append((text, {\"entities\" : entities}))\n",
    "\n",
    "        print(training_data)\n",
    "\n",
    "        with open(output_file, 'wb') as fp:\n",
    "            pickle.dump(training_data, fp)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.exception(\"Unable to process \" + input_file + \"\\n\" + \"error = \" + str(e))\n",
    "        return None\n",
    "if __name__ == '__main__':\n",
    "    plac.call(main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T03:30:14.631848Z",
     "start_time": "2020-05-27T03:30:14.617443Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_csv.reader' object has no attribute 'next'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-95b2f2a109a9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcsvfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mcsvreader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsvfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mfields\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsvreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mfields\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: '_csv.reader' object has no attribute 'next'"
     ]
    }
   ],
   "source": [
    "filename = r\"Data/ner_dataset_nlp_corpus.csv\"\n",
    "with open(filename, 'r') as csvfile:\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    fields = csvreader.next()\n",
    "    fields"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
